{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c654cd91-87fd-42b9-bb71-4952fc416011",
   "metadata": {},
   "source": [
    "# Feature Selection Techniques with Examples\n",
    "\n",
    "In machine learning, Feature selection is the process of choosing variables that are useful in predicting the response (Y). It is considered a good practice to identify which features are important when building predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2260937f-d96c-4c2e-9192-76547cf11e95",
   "metadata": {},
   "source": [
    "## Removing features with low variance\n",
    "\n",
    "VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\n",
    "\n",
    "As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by\n",
    "\n",
    "> **Var[X] = p * (1-p)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0273d382-2072-4d47-bc05-5ce502636a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(0.8 * (1 - 0.8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf25c122-3153-4c0d-94b7-a6e8d7be6c03",
   "metadata": {},
   "source": [
    "As expected, VarianceThreshold has removed the first column, which has a probability (5/6) = 0.8  of containing a zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d953e832-b6d2-4b65-b608-40bfbc3d32d2",
   "metadata": {},
   "source": [
    "# Univariate feature selection\n",
    "\n",
    "Statistical tests can be used to select those features that have the strongest relationship with the output variable.\n",
    "\n",
    "The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n",
    "\n",
    "- SelectKBest removes all but the  highest scoring features\n",
    "- SelectPercentile removes all but a user-specified highest scoring percentage of features\n",
    "\n",
    "Many different statistical test scan be used with this selection method. For example the ANOVA F-value method is appropriate for numerical inputs and categorical data.\n",
    "\n",
    "For instance, we can perform a test to the samples to retrieve only the two best features as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d10ce0a-3e7f-45fc-8d73-17b2d967b767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 39.67  213.162   3.257   4.304  13.281  71.772  23.871  46.141]\n",
      "[[  6.  148.   33.6  50. ]\n",
      " [  1.   85.   26.6  31. ]\n",
      " [  8.  183.   23.3  32. ]\n",
      " [  1.   89.   28.1  21. ]\n",
      " [  0.  137.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection with Univariate Statistical Tests\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# load data\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=f_classif, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "\n",
    "features = fit.transform(X)\n",
    "\n",
    "# You can see the scores for each attribute and the 4 attributes chosen (those with the highest scores).\n",
    "# Specifically features with indexes 0 (preq), 1 (plas), 5 (mass), and 7 (age).\n",
    "\n",
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37869119-ec80-49f1-8f7f-4168b692a780",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.\n",
    "\n",
    "In the example below we construct a ExtraTreesClassifier classifier for the Pima Indians onset of diabetes dataset. You can learn more about the ExtraTreesClassifier class in the scikit-learn API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee770efa-d17c-4aeb-92cd-0ab8be9202a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.117 0.23  0.104 0.085 0.075 0.132 0.113 0.145]\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance with Extra Trees Classifier\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# load data\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier(n_estimators=10)\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eccabbd-1c1a-4262-aa26-0e90585deaf9",
   "metadata": {},
   "source": [
    "# Recursive feature elimination\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute (such as coef_, feature_importances_) or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "\n",
    "RFECV performs RFE in a cross-validation loop to find the optimal number of features.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddecd1c5-323d-4298-ae83-54c0c0c85aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2 0.719 (0.049)\n",
      ">3 0.820 (0.035)\n",
      ">4 0.879 (0.031)\n",
      ">5 0.884 (0.033)\n",
      ">6 0.883 (0.028)\n",
      ">7 0.889 (0.028)\n",
      ">8 0.890 (0.028)\n",
      ">9 0.886 (0.025)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# explore the number of selected features for RFE\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "    return X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for i in range(2, 10):\n",
    "        rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
    "        model = DecisionTreeClassifier()\n",
    "        models[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    return models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc9dfc9-219f-41c9-9f99-46d5a1dc4cc8",
   "metadata": {},
   "source": [
    "#### Which Features Were Selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "612d7c2c-a885-4d27-9b59-0512be931df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected False, Rank: 4.000\n",
      "Column: 1, Selected False, Rank: 5.000\n",
      "Column: 2, Selected True, Rank: 1.000\n",
      "Column: 3, Selected True, Rank: 1.000\n",
      "Column: 4, Selected True, Rank: 1.000\n",
      "Column: 5, Selected False, Rank: 6.000\n",
      "Column: 6, Selected True, Rank: 1.000\n",
      "Column: 7, Selected False, Rank: 2.000\n",
      "Column: 8, Selected True, Rank: 1.000\n",
      "Column: 9, Selected False, Rank: 3.000\n"
     ]
    }
   ],
   "source": [
    "# report which features were selected by RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# define RFE\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "# summarize all features\n",
    "for i in range(X.shape[1]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d10f2d-d588-4ff3-85b6-e0a17919124e",
   "metadata": {},
   "source": [
    "# Sequential feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1eb18a1-af69-4e35-9a25-6a3856f7f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f77fc6d-797a-47d4-953d-e3d4fb31289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, total serum cholesterol\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, total cholesterol / HDL\n",
      "      - s5      ltg, possibly log of serum triglycerides level\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
     ]
    }
   ],
   "source": [
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fd6ddd-3b69-4879-a037-0583698bf2ca",
   "metadata": {},
   "source": [
    "### Feature importance from coefficients\n",
    "The features with the highest absolute coef_ value are considered the most important. We can observe the coefficients directly without needing to scale them (or scale the data) because from the description above, we know that the features were already standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca3bbd2d-ee53-4f39-a665-73f17e26be64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso = LassoCV().fit(X, y)\n",
    "importance = np.abs(lasso.coef_)\n",
    "feature_names = np.array(diabetes.feature_names)\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158fed4-93e2-46f5-87a3-0b4251d8a19a",
   "metadata": {},
   "source": [
    "## Selecting features based on importance\n",
    "Now we want to select the two features which are the most important according to the coefficients.\n",
    "\n",
    "**sklearn.feature_selection.SelectFromModel** accepts a threshold parameter and will select the features whose importance (defined by the coefficients) are above this threshold.\n",
    "\n",
    "Since we want to select only 2 features, we will set this threshold slightly above the coefficient of third most important feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b0f68d5-ae7b-4348-a7ec-1f804b6a9e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by SelectFromModel: ['s1' 's5']\n",
      "Done in 0.198s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "threshold = np.sort(importance)[-3] + 0.01\n",
    "\n",
    "sfm = SelectFromModel(lasso, threshold=threshold).fit(X, y)\n",
    "\n",
    "print(\"Features selected by SelectFromModel: \" f\"{feature_names[sfm.get_support()]}\")\n",
    "print(f\"Done in {toc - tic:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa1810-d30f-4d83-9dce-7177ba57f688",
   "metadata": {},
   "source": [
    "## Selecting features with Sequential Feature Selection\n",
    "Another way of selecting features is to use **sklearn.feature_selection.SequentialFeatureSelector (SFS)**.\n",
    "\n",
    "SFS is a greedy procedure where, at each iteration, we choose the best new feature to add to our selected features based a cross-validation score. That is, we start with 0 features and choose the best single feature with the highest score. The procedure is repeated until we reach the desired number of selected features.\n",
    "\n",
    "We can also go in the reverse direction (backward SFS), i.e. start with all the features and greedily choose features to remove one by one. We illustrate both approaches here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b25f98af-1772-4dbc-a941-1103a8b299d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by forward sequential selection: ['bmi' 's5']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Forward Selection\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    lasso, \n",
    "    n_features_to_select=2,\n",
    "    direction='forward'\n",
    ").fit(X, y)\n",
    "\n",
    "print(\"Features selected by forward sequential selection: \" f\"{feature_names[sfs_forward.get_support()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee5296a2-d377-4f77-a2d7-c71a8e15ef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by backward sequential selection: ['bmi' 's5']\n"
     ]
    }
   ],
   "source": [
    "# backward elemination\n",
    "sfs_backward = SequentialFeatureSelector(\n",
    "    lasso, \n",
    "    n_features_to_select=2,\n",
    "    direction='backward'\n",
    ").fit(X, y)\n",
    "\n",
    "print(\"Features selected by backward sequential selection: \" f\"{feature_names[sfs_backward.get_support()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4fc91f-8265-4224-90a5-ef8e195eb723",
   "metadata": {},
   "source": [
    "Interestingly, forward and backward selection have selected the same set of features. In general, this isn't the case and the two methods would lead to different results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a0f09-ab57-4b01-8ade-55beb33c0ef1",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form.\n",
    "\n",
    "Generally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal component in the transformed result.\n",
    "\n",
    "In the example below, we use PCA and select 3 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7ca1159-c50b-4ab5-b142-37a92759cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.889 0.062 0.026]\n",
      "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
      "  -8.168e-04 -1.402e-01]\n",
      " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
      "  -6.400e-04 -1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with PCA\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# load data\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c9e1bb-74b2-4e90-88cb-26fb7382ea6c",
   "metadata": {},
   "source": [
    "## Feature Selection Checklist\n",
    "\n",
    "1. Do you have domain knowledge? If yes, construct a better set of ad hoc”” features\n",
    "2. Are your features commensurate? If no, consider normalizing them.\n",
    "3. Do you suspect interdependence of features? If yes, expand your feature set by constructing conjunctive features or products of features, as much as your computer resources allow you.\n",
    "4. Do you need to prune the input variables (e.g. for cost, speed or data understanding reasons)? If no, construct disjunctive features or weighted sums of feature\n",
    "5. Do you need to assess features individually (e.g. to understand their influence on the system or because their number is so large that you need to do a first filtering)? If yes, use a variable ranking method; else, do it anyway to get baseline results.\n",
    "6. Do you need a predictor? If no, stop\n",
    "7. Do you suspect your data is “dirty” (has a few meaningless input patterns and/or noisy outputs or wrong class labels)? If yes, detect the outlier examples using the top ranking variables obtained in step 5 as representation; check and/or discard them.\n",
    "8. Do you know what to try first? If no, use a linear predictor. Use a forward selection method with the “probe” method as a stopping criterion or use the 0-norm embedded method for comparison, following the ranking of step 5, construct a sequence of predictors of same nature using increasing subsets of features. Can you match or improve performance with a smaller subset? If yes, try a non-linear predictor with that subset.\n",
    "9. Do you have new ideas, time, computational resources, and enough examples? If yes, compare several feature selection methods, including your new idea, correlation coefficients, backward selection and embedded methods. Use linear and non-linear predictors. Select the best approach with model selection\n",
    "10. Do you want a stable solution (to improve performance and/or understanding)? If yes, subsample your data and redo your analysis for several “bootstrap”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabeaec9-83f5-4e3f-b210-f2bf74b9337e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
